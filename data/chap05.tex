\chapter{总结与未来展望}
\label{cha:conclusion}

\section{工作总结}

回顾本篇论文，从供电网络的基础数学模型出发，针对求解矩阵的稀疏特征与数学性质，使用了合适的预条件子，而且找到了适合于求解稀疏矩阵的并行化算法。
根据提出的算法，我们用C++语言实现了相应的程序，并在多机集群上进行了实验。
利用C++语言的速度优势与一些开源代码库的帮助，问题得到了很好的解决，同时得以探索了一些改善算法的可能性。

本文关注的重点在于：由于系数矩阵的规模十分巨大，多重网格预条件子在保证降低系数矩阵的条件数的同时，因为能控制矩阵粗粒度的原因只增加了少量的计算量，
同样利用矩阵的稀疏性，本文提出的算法很好地利用了多机集群的计算能力：通过消息传递接口（MPI）的通信机制，使得共轭梯度法的计算量能够均摊在各个计算节点上，大大提高了求解的效率。
实验结果也很好地表明了算法的有效性，证明了并行化对算法的提升作用。

\section{未来展望}

本文的工作仍有不少不足和可提高之处，改进的方面主要集中在算法方面。

首先是本文提出的并行化算法的负载平衡（Load Balance）只是根据矩阵行号进行一个简单的分发，没有寻求途径去最小化计算节点间的通讯量。从第四章的实验可以看出，当计算节点的
数量上升后，节点间的通讯的花销也会相应地提升，这体现在通讯量占非零元素个数的比例上（参照表格\ref{tab:tabibmresult} 的第八列）。随着通讯量的不断增加，
节点间通讯花销会逐渐成为算法的瓶颈，抵消并行化算法的优势，大大影响并行化对算法效率的提升。因此下一步工作可以从减少通讯量做起，比如可以像解一个最优化问题一样，通过
最小化跨计算节点的非零系数的数量，来寻找行号分发的合理方案。这其实是一个网络流的最小割问题~\cite{lawler20014, boykov2004experimental}，可以利用一些网络流算法解决。

其次是通信消息的重复性。目前的算法里，对于单个节点而言，如果它本地保存的某些变量值被所有大部分其他节点需要（计算SPVM、向量内积等），那么这个节点需要把这些本地值依次发送给所有其它节点，这样每次计算都需要进行大量重复的沟通，效率是不高的。我们可以尝试把这些常用的变量值存到节点都能够共同访问的空间中，从而降低通信的花销；或者把这些常用的变量直接
分发给所有的节点，让所有的节点都在本地计算这些常用变量的值。当然，虽然这能降低通信的开销，同时也会降低一部分的算法并行性，因此需要更进一步的实验去做性能的权衡，探索其可能性。

并行化在解决供电网络仿真问题的方面已经展现了一些潜力，下一步的工作应该是试验更多的迭代算法，测试不同的预条件子，挖掘他们的并行计算能力。不同的算法与预条件子
适合于不同的数据条件，因此可能有不同的并行化方案出现。
